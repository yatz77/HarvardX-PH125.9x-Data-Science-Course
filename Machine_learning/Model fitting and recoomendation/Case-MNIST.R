library(dslabs)
mnist <- read_mnist()

names(mnist)
dim(mnist$train$images)

class(mnist$train$labels)
table(mnist$train$labels)

# sample 10k rows from training set, 1k rows from test set
set.seed(123, sample.kind = "Rounding")
index <- sample(nrow(mnist$train$images), 10000)
x <- mnist$train$images[index,]
y <- factor(mnist$train$labels[index])

index <- sample(nrow(mnist$test$images), 1000)
#note that the line above is the corrected code - code in video at 0:52 is incorrect
x_test <- mnist$test$images[index,]
y_test <- factor(mnist$test$labels[index])

# predictor preprocessing
install.packages("matrixStats")
library(matrixStats)
sds <- colSds(x)
qplot(sds, bins = 256, color = I("black"))

library(caret)
nzv <- nearZeroVar(x)
image(matrix(1:784 %in% nzv, 28, 28))

col_index <- setdiff(1:ncol(x), nzv)
length(col_index)

# test out random forest and knn algorithms
colnames(x) <- 1:ncol(mnist$train$images)
colnames(x_test) <- colnames(x)

control <- traincontrol(method = "cv", number = 10, p = 0.9)
train_knn <- train(x[,col_index], y,
                   method = "knn",
                   tuneGrid = data.frame(k = c(1,3,4,7)),
                   trControl = control)
ggplot(train_knn)

n <- 1000
b <- 2
index <- sample(nrow(x), n)
control <- trainControl(method = "cv", number = b, p = .9)
train_knn <- train(x[index, col_index], y[index],
                   method = "knn",
                   tuneGrid = data.frame(k = c(3,5,7)),
                   trControl = control)
fit_knn <- knn3(x[ ,col_index], y, k=3)
class(x_test[, col_index])
y_hat_knn <- predict(fit_knn,
                     x_test[, col_index],
                     type="class")
cm <- confusionMatrix(y_hat_knn, factor(y_test))
cm$overall["Accuracy"]

cm$byClass[,1:2]

library(Rborist)
control <- trainControl(method="cv", number = 5, p = 0.8)
grid <- expand.grid(minNode = c(1,5), predFixed = c(19,15,25,35,50))
train_rf <- train(x[, col_index], y,
                  method = "Rborist",
                  nTree = 50,
                  trControl = control,
                  tuneGrid = grid,
                  nSamp = 5000)
ggplot(train_rf)
train_rf$bestTune

fit_rf <- Rborist(x[, col_index], y,
                  nTree = 1000,
                  minNode = train_rf$bestTune$minNode,
                  predFixed = train_rf$bestTune$predFixed)

y_hat_rf <- factor(levels(y)[predict(fit_rf, x_test[ , col_index])$yPred])
cm <- confusionMatrix(y_hat_rf, y_test)
cm$overall["Accuracy"]

install.packages("rafalib")
library(rafalib)
rafalib::mypar(3,4)
for(i in 1:12){
  image(matrix(x_test[i,], 28, 28)[, 28:1],
        main = paste("Our prediction:", y_hat_rf[i]),
        xaxt="n", yaxt="n")
}

# Variable importance
library(randomForest)
x <- mnist$train$images[index,]
y <- factor(mnist$train$labels[index])
rf <- randomForest(x, y, ntree = 50)
imp <- importance(rf)
imp

image(matrix(imp, 28, 28))

p_max <- predict(fit_knn, x_test[,col_index])
p_max
p_max <- apply(p_max, 1, max)
ind <- which(y_hat_rf != y_test)
ind
ind <- ind[order(p_max[ind], decreasing = T)]
ind
rafalib::mypar(3,4)
for(i in ind[1:12]){
  image(matrix(x_test[i,], 28, 28)[, 28:1],
        main = paste0("Pr(", y_hat_rf[i],")=",round(p_max[i], 2),
                      " but is a ",y_test[i]),
        xaxt="n", yaxt="n")
}

# ensembles
p_rf <- predict(fit_rf, x_test[,col_index])$census
p_rf <- p_rf / rowSums(p_rf)
p_knn <- predict(fit_knn, x_test[,col_index])
p <- (p_rf + p_knn)/2
y_pred <- factor(apply(p, 1, which.max)-1)
confusionMatrix(y_pred, y_test)

#COMPREHENSION CHECK
library(tidyverse)
library(caret)
models <- c("glm", "lda", "naive_bayes", "svmLinear",  "knn", "gamLoess", "multinom", "qda", "rf","adaboost")

set.seed(1, sample.kind = "Rounding")
data("mnist_27")

fits <- lapply(models, function(model){
  print(model)
  train(y ~ ., method = model, data = mnist_27$train)
})
1
1
1

names(fits) <- models

# create a matrix of predictions
fits
class(mnist_27$test$x_1[1])
predmatrix <- sapply(fits, function(x){
  predict(x,mnist_27$test[,-1])
}) 
predmatrix
pred <- map(fits, function(object) 
  predict(object, newdata = mnist_27$test))

#accuracy of all the models
  acc <- sapply(pred, function(object){
    confusionMatrix(data = object, reference = mnist_27$test$y)$overall[["Accuracy"]]
  }) 
  acc <- colMeans(predmatrix == mnist_27$test$y)
  acc
  mean(acc)
pred

#building ensemble when more than 50% of models predict 7 predict 7 if not 2
ensemble <- as.factor(ifelse(rowSums(predmatrix == 7)>5, 7, 2))
mean(ensemble == mnist_27$test$y)

#minimum accuracy estimates

acc_hat <- sapply(fits, function(fit) min(fit$results$Accuracy))
mean(acc_hat)

# only use methods with an estimated accuracy greater than or equal to 0.8
acc_hat
highacc_predmatrix <- predmatrix[,c(1,3,5,6,8,9)]
highacc_ensemble <- as.factor(ifelse(rowMeans(highacc_predmatrix==7)>0.5,7,2))
mean(highacc_ensemble == mnist_27$test$y)

#COMPREHENSION CHECK: DIMENSION REDUCTION
library(dslabs)
data("tissue_gene_expression")
dim(tissue_gene_expression$x)

#first two principal components plot

pc <- prcomp(tissue_gene_expression$x)
data.frame(pc_1 = pc$x[,1], pc_2 = pc$x[,2], 
           tissue = tissue_gene_expression$y) %>%
  ggplot(aes(pc_1, pc_2, color = tissue)) +
  geom_point()
pc
